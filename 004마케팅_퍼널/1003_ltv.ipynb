{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CRM 분석이란 무엇인가?\n",
    "\n",
    "# CRM 분석은 귀사의 판매 및 고객 서비스 성과를 입증하는 데이터입니다. 또한 CRM 분석은 보다 현명한 비즈니스 의사 결정을 알리는 데 사용할 수 있는 고객 데이터를 제공합니다. 일반적으로 CRM 소프트웨어를 사용하여 CRM 분석을 얻고 모든 데이터 수집 및 보고서 생성을 자동화합니다.\n",
    "\n",
    "# CRM 분석의 이점\n",
    "\n",
    "# CRM 분석의 주요 이점은 이를 사용하여 영업, 고객 서비스 및 마케팅 프로세스를 알릴 수 있다는 것입니다. CRM 분석을 사용하여 다음을 통해 방법을 개선할 수 있습니다.\n",
    "\n",
    "# 고객 서비스 평가. CRM 분석은 고객 서비스 팀의 성과를 알려줍니다. 팀이 개선할 수 있는 수치를 발견하면, 이러한 목표를 향해 팀을 추진하는 관행을 구현하십시오.\n",
    "# 정확한 고객 데이터. 고객 데이터를 인구 통계 마케팅 또는 전자 메일 마케팅에 사용하든, 적합한 사람에게 연락하고 있는지 알아야 합니다. CRM 분석은 당신이 그것을 하고 있다는 것을 보장한다.\n",
    "# 철저한 고객 분석. 당신의 고객은 보통 한 분기당 얼마를 소비합니까? 같은 제품을 몇 번이고 사는 건가요, 아니면 다른 건가요? CRM 분석을 통해 이러한 질문에 대한 확실한 답을 얻을 수 있으며, 배운 내용을 사용하여 마케팅 전략을 개선할 수 있습니다.\n",
    "# 효율적인 리드 생성. 당신의 CRM 분석은 당신의 마케팅 노력 중 어떤 것이 구매와 가장 강하게 관련이 있는지 알려줄 수 있다. 구매와 밀접한 관련이 있는 한 가지 접근 방식이 있지만 이러한 접근 방식을 통해 일부 고객만을 대상으로 했다면, 이 방법을 더 시도해 보십시오. 즉, 매출이 증가할 수 있습니다.\n",
    "# 데이터 셋 설명 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from lifetimes.plotting import *\n",
    "from lifetimes.utils import *\n",
    "from lifetimes import BetaGeoFitter\n",
    "from lifetimes.fitters.gamma_gamma_fitter import GammaGammaFitter\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, rand, SparkTrials, STATUS_OK, space_eval, Trials\n",
    "# gridsearch와 유사한 머신러닝 하이퍼파라미터 튜닝\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0         536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1         536365     71053                  WHITE METAL LANTERN         6   \n",
      "2         536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3         536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4         536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "...          ...       ...                                  ...       ...   \n",
      "541904    581587     22613          PACK OF 20 SPACEBOY NAPKINS        12   \n",
      "541905    581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
      "541906    581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
      "541907    581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
      "541908    581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
      "\n",
      "               InvoiceDate  UnitPrice CustomerID         Country  \n",
      "0      2010-12-01 08:26:00       2.55      17850  United Kingdom  \n",
      "1      2010-12-01 08:26:00       3.39      17850  United Kingdom  \n",
      "2      2010-12-01 08:26:00       2.75      17850  United Kingdom  \n",
      "3      2010-12-01 08:26:00       3.39      17850  United Kingdom  \n",
      "4      2010-12-01 08:26:00       3.39      17850  United Kingdom  \n",
      "...                    ...        ...        ...             ...  \n",
      "541904 2011-12-09 12:50:00       0.85      12680          France  \n",
      "541905 2011-12-09 12:50:00       2.10      12680          France  \n",
      "541906 2011-12-09 12:50:00       4.15      12680          France  \n",
      "541907 2011-12-09 12:50:00       4.15      12680          France  \n",
      "541908 2011-12-09 12:50:00       4.95      12680          France  \n",
      "\n",
      "[541909 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset =  pd.read_csv('/Users/hyeonchanglee/Documents/data_analyst/004마케팅_퍼널/CRM_dataset/CRM_dataset.csv', \\\n",
    "    encoding = 'unicode_escape',dtype = {'CustomerID': str,'InvoiceID': str},)\n",
    "\n",
    "dataset['InvoiceDate'] = pd.to_datetime(dataset['InvoiceDate'])\n",
    "\n",
    "df = dataset.copy()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(),  df.duplicated().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "count    392731.000000\n",
      "mean         13.153746\n",
      "std         181.588650\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           6.000000\n",
      "75%          12.000000\n",
      "max       80995.000000\n",
      "Name: Quantity, dtype: float64\n",
      "Empty DataFrame\n",
      "Columns: [InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate']).dt.date\n",
    "\n",
    "# print(df['InvoiceDate'])\n",
    "\n",
    "#null\n",
    "# df.isnull().sum()\n",
    "df.dropna(inplace=True) #널값 제거\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "#duplicates\n",
    "df.drop_duplicates(inplace=True) #중복 제거\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "#cancel\n",
    "df = df[df['Quantity']>0] # 0 이하 제거\n",
    "df = df[~df['InvoiceNo'].str.contains('C',na=False)] # C 값 제거\n",
    "\n",
    "print(df['Quantity'].describe())\n",
    "print(df[df['InvoiceNo'].str.contains('C')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sales'] = df['Quantity'] * df['UnitPrice'] # Sales생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CustomerID InvoiceNo InvoiceDate  Sales\n",
      "0           17850    536365  2010-12-01  15.30\n",
      "1           17850    536365  2010-12-01  20.34\n",
      "2           17850    536365  2010-12-01  22.00\n",
      "3           17850    536365  2010-12-01  20.34\n",
      "4           17850    536365  2010-12-01  20.34\n",
      "...           ...       ...         ...    ...\n",
      "541904      12680    581587  2011-12-09  10.20\n",
      "541905      12680    581587  2011-12-09  12.60\n",
      "541906      12680    581587  2011-12-09  16.60\n",
      "541907      12680    581587  2011-12-09  16.60\n",
      "541908      12680    581587  2011-12-09  14.85\n",
      "\n",
      "[392731 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "cols = ['CustomerID','InvoiceNo','InvoiceDate','Sales']\n",
    "df = df[cols]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>recency</th>\n",
       "      <th>T</th>\n",
       "      <th>monetary_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12346</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>6.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>599.701667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>3.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>301.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12349</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency  recency      T  monetary_value\n",
       "CustomerID                                           \n",
       "12346             0.0      0.0  325.0        0.000000\n",
       "12347             6.0    365.0  367.0      599.701667\n",
       "12348             3.0    283.0  358.0      301.480000\n",
       "12349             0.0      0.0   18.0        0.000000\n",
       "12350             0.0      0.0  310.0        0.000000"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RFMT를 쉽게 계산해주는 함수\n",
    "# from lifetimes.utils import *\n",
    "# summary_data_from_transaction_data\n",
    "\n",
    "current_date = df['InvoiceDate'].max() #2011-12-09\n",
    "\n",
    "\n",
    "metrics_df = summary_data_from_transaction_data(df,\n",
    "                                    customer_id_col='CustomerID', # 고객아이디\n",
    "                                    datetime_col='InvoiceDate',   # 일자\n",
    "                                    monetary_value_col='Sales',   # 매출\n",
    "                                    observation_period_end=current_date) # 현재날짜\n",
    "\n",
    "metrics_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# frequency           6.000000\n",
    "# recency           365.000000\n",
    "# T                 367.000000\n",
    "# monetary_value    599.701667\n",
    "# Name: 12347, dtype: float64\n",
    "\n",
    "\n",
    "# 아이디 12347은 \n",
    "# 총 6번 구매했고 (frequency),\n",
    "# 마지막 구매일 - 첫 구매일은 365일 (recency),\n",
    "# 집계일 - 첫 구매일은 367일 (T)이며,\n",
    "# 평균 구매 금액은 £600 정도 (monetary_value)임을 의미합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sales    599.701667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('CustomerID').get_group('12347')['InvoiceNo'].nunique()\n",
    "\n",
    "data_12347 = df.groupby('CustomerID').get_group('12347')\n",
    "# data_12347.groupby('InvoiceNo')['InvoiceDate'].count()\n",
    "\n",
    "data_12347.groupby('InvoiceDate').sum()[1:].mean() # 날짜별로 그룹화 한 후 구매금액의 합을 날짜별 평균을 구함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Customer Lifetime Value\n",
    "\n",
    "\n",
    "# cltv_df = df.groupby('CustomerID').agg({\n",
    "#     'InvoiceDate': [lambda x : (x.max() - x.min()).days,\n",
    "#                     lambda x : (today_date - x.min()).days], # 이런식으로 리스트 안에 두가지 식을 적용 시 컬럼이 각각 만들어짐\n",
    "#     'InvoiceNo': lambda x : x.nunique(),\n",
    "#     'TotalPrice' : lambda x : x.sum()\n",
    "# })\n",
    "\n",
    "# # cltv_df.head()\n",
    "# # InvoiceDate\tInvoiceNo\tTotalPrice\n",
    "# # <lambda_0>\t<lambda_1>\t<lambda>\t<lambda>\n",
    "\n",
    "# # T 컬럼은 오늘부터 가장 처음 구매한 기간의 차로 구매 기간을 의미\n",
    "# cltv_df.columns = cltv_df.columns.droplevel(0)\n",
    "# cltv_df.columns = ['recency', 'T', 'frequency', 'monetary']\n",
    "# cltv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerID  InvoiceDate\n",
       "12346       2011-01-18     77183.60\n",
       "12347       2010-12-07       711.79\n",
       "            2011-01-26       475.39\n",
       "            2011-04-07       636.25\n",
       "            2011-06-09       382.52\n",
       "                             ...   \n",
       "18283       2011-11-30       220.31\n",
       "            2011-12-06       208.00\n",
       "18287       2011-05-22       765.28\n",
       "            2011-10-12      1001.32\n",
       "            2011-10-28        70.68\n",
       "Name: Sales, Length: 16766, dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "map_money = df.groupby(['CustomerID','InvoiceDate'])['Sales'].sum()  #고객 그룹 -> 일자별 그룹 매출합.\n",
    "\n",
    "# map_money.groupby('CustomerID').agg(lambda x : )\n",
    "\n",
    "# for e,i in tqdm(enumerate(map_money.index)):\n",
    "#     if e == 20:\n",
    "#         break\n",
    "\n",
    "#     map_money = df.groupby(['CustomerID','InvoiceDate'])['Sales'].sum()\n",
    "#     cos_x = map_money.groupby('CustomerID').get_group(i[0])\n",
    "#     # print(cos_x)\n",
    "#     cos_x.iloc[0] = 0\n",
    "#     # print(cos_x)\n",
    "\n",
    "# print(map_money)\n",
    "\n",
    "\n",
    "\n",
    "# for i in tqdm(map_money.index):\n",
    "#     map_money = df.groupby(['CustomerID','InvoiceDate'])['Sales'].sum()\n",
    "#     cos_x = map_money.groupby('CustomerID').get_group(i[0])\n",
    "#     # print(cos_x)\n",
    "#     cos_x.iloc[0] = 0\n",
    "#     # print(cos_x)\n",
    "\n",
    "# print(map_money)\n",
    "\n",
    "\n",
    "# map_money.groupby('InvoiceDate').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalizer_coef\n",
    "\n",
    "\n",
    "# model = BetaGeoFitter(penalizer_coef=l2_reg)\n",
    "# model = GammaGammaFitter(penalizer_coef=l2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 /테스트 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency_cal</th>\n",
       "      <th>recency_cal</th>\n",
       "      <th>T_cal</th>\n",
       "      <th>monetary_value_cal</th>\n",
       "      <th>frequency_holdout</th>\n",
       "      <th>monetary_value_holdout</th>\n",
       "      <th>duration_holdout</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12346</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>4.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>519.7675</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.192069</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>2.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>297.2200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103.333333</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12352</th>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>421.7700</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.090000</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18280</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18281</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.568000</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18283</th>\n",
       "      <td>8.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>122.2975</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.227946</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18287</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.146341</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3370 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency_cal  recency_cal  T_cal  monetary_value_cal  \\\n",
       "CustomerID                                                          \n",
       "12346                 0.0          0.0  235.0              0.0000   \n",
       "12347                 4.0        238.0  277.0            519.7675   \n",
       "12348                 2.0        110.0  268.0            297.2200   \n",
       "12350                 0.0          0.0  220.0              0.0000   \n",
       "12352                 3.0         34.0  206.0            421.7700   \n",
       "...                   ...          ...    ...                 ...   \n",
       "18280                 0.0          0.0  187.0              0.0000   \n",
       "18281                 0.0          0.0   90.0              0.0000   \n",
       "18282                 0.0          0.0   36.0              0.0000   \n",
       "18283                 8.0        242.0  247.0            122.2975   \n",
       "18287                 0.0          0.0  111.0              0.0000   \n",
       "\n",
       "            frequency_holdout  monetary_value_holdout  duration_holdout  \n",
       "CustomerID                                                               \n",
       "12346                     0.0                0.000000              90.0  \n",
       "12347                     2.0               26.192069              90.0  \n",
       "12348                     1.0              103.333333              90.0  \n",
       "12350                     0.0                0.000000              90.0  \n",
       "12352                     3.0               20.090000              90.0  \n",
       "...                       ...                     ...               ...  \n",
       "18280                     0.0                0.000000              90.0  \n",
       "18281                     0.0                0.000000              90.0  \n",
       "18282                     1.0               15.568000              90.0  \n",
       "18283                     5.0                3.227946              90.0  \n",
       "18287                     2.0               26.146341              90.0  \n",
       "\n",
       "[3370 rows x 7 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# calibration_and_holdout_data  함수\n",
    "# calibration 을 훈련 데이터라고 함\n",
    "# holdout 을 테스트 데이터라고 함 (2:1 혹은 마지막 3달을 둠)\n",
    "\n",
    "holdout_days = 90\n",
    "calibration_end_date = current_date - timedelta(days = holdout_days) \n",
    "current_date = df['InvoiceDate'].max() #2011-12-09\n",
    "\n",
    "# 현재 날짜에서 90일을 뺀 날짜를 계산해줌\n",
    "\n",
    "# print(calibration_end_date)\n",
    "\n",
    "metrics_cal_df = calibration_and_holdout_data(df,\n",
    "                                            customer_id_col='CustomerID',\n",
    "                                            datetime_col='InvoiceDate',\n",
    "                                            calibration_period_end=calibration_end_date,\n",
    "                                            observation_period_end=current_date,\n",
    "                                            monetary_value_col='Sales')\n",
    "\n",
    "\n",
    "# customer_id_col='Customer' # 고객 아이디가 있는 컬럼명\n",
    "# datetime_col='InvoiceDate', # 일자가 있는 컬럼명\n",
    "# calibration_period_end=calibration_end_date, # 훈련 데이터가 끝난는 날짜\n",
    "# observation_period_end=current_date, # 데이터가 끝나는 날짜\n",
    "# monetary_value_col='Sales') # 금액이 있는 컬럼명\n",
    "metrics_cal_df\n",
    "\n",
    "\n",
    "# calibration 기간 동안의 RFMT를 계산한 값은 _cal이 붙고, \n",
    "# holdout 기간 동안의 F와 M을 계산한 값은 _holdout이 붙습니다.\n",
    "# duration_holdout은 holdout data이 며칠인지 나타내는 컬럼입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 데이터 만들기\n",
    "# 고객별 RFMT가 들어간 데이터에서 frequency가 1 이상인 데이터만 가져오기!\n",
    "\n",
    "# 아래 코드에서 보면 frequency가 0인 것은 제외하고 가져오는데, \n",
    "# 이 고객들은 전체 기간 동안 구매 일수가 1일인 유저입니다. (frequency는 총 구매일수 - 1) \n",
    "# 이들은 “반복적인” 구매를 한 고객들이 아니라서 \n",
    "# BG/NBD 모형 가정에서 벗어난 고객들이기 때문에 제외하고 적합해야 합니다.\n",
    "\n",
    "whole_filtered_df = metrics_df[metrics_df.frequency > 0]\n",
    "filtered_df = metrics_cal_df[metrics_cal_df.frequency_cal > 0]\n",
    "\n",
    "# whole_filtered_df는 L2 페널티를 최적화한 후에\n",
    "# 제일 마지막에 LTV를 계산할 때 쓸 데이터이고 (calibration / holdout을 나누지 않은 데이터)\n",
    "# filtered_df는 L2 페널티를 최적화하기 위해 calibration / holdout을 나눈 데이터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 페널티 최적화하기\n",
    "\n",
    "# hyperopt 모듈의 fmin 함수\n",
    "\n",
    "# score_model: 실제값과 예측값의 차이에 대한 지표 (MSE / RMSE/ MAE)를 계산하는 함수\n",
    "# evaluate_bgnbd_model: calibration data와 l2_reg를 넣어 BG/NBD 모형을 적합시키고, holdout data의 구매 일수 (frequency)에 대한 실제값과 예측값에 대한 MSE를 계산하는 함수\n",
    "# evaluate_gg_model: calibration data와 l2_reg를 넣어 Gamma-Gamma 모형을 적합시키고, holdout data의 평균 구매 금액 (monetary value)에 대한 실제값과 예측값에 대한 MSE를 계산하는 함수\n",
    "\n",
    "\n",
    "def score_model(actuals, predicted, metric = 'mse'): # score 평가\n",
    "\n",
    "    metric = metric.lower() # 입력값을 소문자로 받음\n",
    "    \n",
    "    # MSE/RMSE\n",
    "    if metric == 'mse' or metric == 'rmse':\n",
    "        val = np.sum(np.square(actuals - predicted))/actuals.shape[0]\n",
    "    elif metric == 'rmse':\n",
    "        val = np.sqrt(val)\n",
    "\n",
    "    # MAE\n",
    "    elif metric == 'mae':\n",
    "        val = np.sum(np.abs(actuals-predicted))/actuals.shape[0]\n",
    "    else:\n",
    "        val = None\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "def evaluate_bgnbd_model(param): # bgnbd모델\n",
    "\n",
    "    data = inputs \n",
    "    l2_reg = param\n",
    "\n",
    "\n",
    "    # 모형 적합\n",
    "    model = BetaGeoFitter(penalizer_coef=l2_reg) # l2규제 적용, 과대적합 방지\n",
    "    model.fit(data['frequency_cal'], data['recency_cal'], data['T_cal'])\n",
    "    # fit 시 빈도, 최근, T전체기간 을 입력.\n",
    "    \n",
    "\n",
    "    # 모형 평가\n",
    "    frequency_actual = data['frequency_holdout'] # 실제값\n",
    "    frequency_predicted = model.predict(data['duration_holdout'], # 예측값\n",
    "                                        data['frequency_cal'],\n",
    "                                        data['recency_cal'],\n",
    "                                        data['T_cal'])\n",
    "\n",
    "    mse = score_model(frequency_actual, frequency_predicted, 'mse')\n",
    "\n",
    "    return {'loss' : mse, 'status' : STATUS_OK}\n",
    "\n",
    "\n",
    "def evaluate_gg_model(param): # gg모델\n",
    "\n",
    "    data = inputs \n",
    "    l2_reg = param\n",
    "\n",
    "    #GammaGamma 모형 적합\n",
    "    model = GammaGammaFitter(penalizer_coef=l2_reg)\n",
    "    model.fit(data['frequency_cal'], data['monetary_value_cal'])\n",
    "\n",
    "    # 모형 평가\n",
    "    monetary_actual = data['monetary_value_holdout'] # 실제값\n",
    "    monetary_predicted = model.conditional_expected_average_profit(\n",
    "                                            data['frequency_holdout'],\n",
    "                                            data['monetary_value_holdout']\n",
    "                                            )\n",
    "    mse = score_model(monetary_actual, monetary_predicted, 'mse')\n",
    "\n",
    "    # return score and status\n",
    "    return {'loss' : mse, 'status' : STATUS_OK }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 18.75trial/s, best loss: 2.9943923609709566]\n",
      "0.9998408311469851\n"
     ]
    }
   ],
   "source": [
    "# BG/NBD\n",
    "\n",
    "# beta-geometric 베타분포 기하분포의 결합\n",
    "# negative-binomial-distributiond 음이항함수\n",
    "\n",
    "\n",
    "# 이 후 hyperopt 모듈의 fmin 함수를 이용해 \n",
    "# BG/NBD 모형과 Gamma-Gamma 모형 각각의 최적의 L2 penalty를 찾습니다.\n",
    "\n",
    "\n",
    "\n",
    "search_space = hp.uniform('l2',0.0,1.0) #????\n",
    "algo  = tpe.suggest # ??\n",
    "trials = Trials() # ??\n",
    "inputs = filtered_df  # l2페널티 최적화를 위해 훈련/테스트로 분리한 데이터셋\n",
    "\n",
    "argmin = fmin(\n",
    "    fn = evaluate_bgnbd_model, # 목적함수\n",
    "    space = search_space,\n",
    "    algo = algo,\n",
    "    max_evals = 100,\n",
    "    trials = trials\n",
    ")\n",
    "\n",
    "l2_bgnbd = space_eval(search_space, argmin) # ?\n",
    "print(l2_bgnbd)\n",
    "\n",
    "\n",
    "# 0.9998408311469851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.57trial/s, best loss: 428.5688405144764]\n",
      "0.007279394874148898\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "#GammaGamma\n",
    "\n",
    "argmin = fmin(\n",
    "    fn = evaluate_gg_model,\n",
    "    space = search_space,\n",
    "    algo = algo,\n",
    "    max_evals = 100,\n",
    "    trials = trials\n",
    ")\n",
    "\n",
    "l2_gg = space_eval(search_space, argmin)\n",
    "print(l2_gg)\n",
    "\n",
    "# 0.007279394874148898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BG/NBD 모형 적합\n",
    "\n",
    "# BetaGeoFitter (penalizer_coef) 및 predict 함수\n",
    "\n",
    "\n",
    "lifetimes_model = BetaGeoFitter(penalizer_coef =l2_bgnbd) # BG/NBD l2규제 #l2_bgnbd = hyperopt로 나온 결과\n",
    "\n",
    "# calibration 데이터의 R, F, T로 모형 적합\n",
    "lifetimes_model.fit(filtered_df['frequency_cal'], filtered_df['recency_cal'], filtered_df['T_cal'])\n",
    "\n",
    "# holdout 데이터로 모델 평가 : F의 실제 값과 예측값의 MSE\n",
    "frequency_actual = filtered_Df['frequency_holdout']\n",
    "frequen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            frequent  recent    T      monetary\n",
      "CustomerID                                     \n",
      "12346              0       0  325  77183.600000\n",
      "12347              6     365  367     23.681319\n",
      "12348              3     283  358     57.975484\n",
      "12349              0       0   18     24.076027\n",
      "12350              0       0  310     19.670588\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "current_date = df['InvoiceDate'].max() #2011-12-09\n",
    "# cols = ['CustomerID','InvoiceNo','InvoiceDate','Sales']\n",
    "\n",
    "\n",
    "\n",
    "# 실제로 만드는 방법\n",
    "rfm_df = df.groupby('CustomerID').agg({\n",
    "            'InvoiceNo': lambda x : x.nunique() -1, # frequent 첫 거래는 제거\n",
    "            'InvoiceDate':[\n",
    "                            lambda x  : (x.max() - x.min()).days,  # recent                          \n",
    "                            lambda x : (current_date - x.min()).days # T\n",
    "                            ], \n",
    "            'Sales': lambda x : x.mean()\n",
    "                                            })\n",
    "\n",
    "# data_12347.groupby('InvoiceDate').sum()[1:].mean() # 날짜별로 그룹화 한 후 구매금액의 합을 날짜별 평균을 구함\n",
    "\n",
    "rfm_df.columns = ['frequent','recent','T','monetary']\n",
    "\n",
    "print(rfm_df.head())\n",
    "\n",
    "# R : 고객별 첫 구매 ~ 마지막 구매까지의 시간\n",
    "# F : 고객별 구매 일 수 (첫구매는 포함하지 않는다.)\n",
    "# M : 고객별 평균 구매금액(날짜별)\n",
    "# T : 고객별 첫 구매 ~ 집계일까지의 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM Analysis\n",
    "\n",
    "RFM은 Recency, Frequency 및 Money value를 나타내며, 각각은 몇 가지 주요 고객 특성에 해당합니다. 이러한 RFM 측정 기준은 빈도와 금전적 가치가 고객의 평생 가치에 영향을 미치고, 근래에는 참여의 척도인 유지에 영향을 미치기 때문에 고객의 행동을 나타내는 중요한 지표입니다.\n",
    "\n",
    "RFM 요인은 다음과 같은 사실을 보여줍니다.\n",
    "\n",
    "최근 구매일수록 고객이 판촉 활동에 더 많은 반응을 보입니다.\n",
    "고객이 더 자주 구매할수록 참여도와 만족도가 높아집니다.\n",
    "금전적 가치는 많은 소비자와 낮은 가치의 구매자를 구별합니다.\n",
    "\n",
    "최근 자주 돈을 많이 쓴 고객\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['InvoiceDate'].min())\n",
    "print(df['InvoiceDate'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = dt.datetime(2011,12,11)\n",
    "# print(today_date) #2011-12-11 00:00:00 오늘이라고 가정, 데이터의 마지막날 하루 뒤\n",
    "\n",
    "rfm = df.groupby('CustomerID').agg({\n",
    "    'InvoiceDate' : lambda x : (today_date - x.max()).days,\n",
    "    'InvoiceNo' : lambda x : x.nunique(),\n",
    "    'TotalPrice' : lambda x : x.sum()\n",
    "})\n",
    "\n",
    "# 고객 아이디로 그룹화\n",
    "# 송장 일자를 오늘 날자에서 최대 날짜(마지막 구매날짜)를 뺀 날짜로, 3이면 최근으로부터 3일전에 구매함을 의미 (최근의 정의)\n",
    "# 송장 번호를 유니크한 개수로, 5 이면 해당 기간동안 5개의 별도의 주문을 함을 의미 (빈도의 정의)\n",
    "# 구매 금액을 다 더함 (금전가치의 정의)\n",
    "\n",
    "rfm.columns = ['recency','frequency','monetary']\n",
    "# rfm['monetary'] = rfm[rfm['monetary'] > 0]\n",
    "\n",
    "rfm = rfm.reset_index()\n",
    "\n",
    "print(rfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfm['monetary'].sort_values(ascending =False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfm_scores(dataframe):\n",
    "    \n",
    "    df_ = dataframe.copy()\n",
    "\n",
    "    #qcut 은 데이터프레임 내의 데이터를 동일한 숫자로 등분함 # 5등분 \n",
    "    # 5점 ~ 1점으로 스코어제, labels에 따라 작은값->큰값으로 0번째 레이블, 1번째.. 로 매김\n",
    "    # 최근 점수\n",
    "    df_['recency_score'] = pd.qcut(df_['recency'],5,labels=[5,4,3,2,1])\n",
    "\n",
    "    # df['a'].rank()함수는  method=first는 동점 관측치 중 먼저 나타나는 값을 높은 순위를 줌\n",
    "    # 빈도 점수\n",
    "    df_['frequency_score'] = pd.qcut(df_['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    # 금전 가치 점수\n",
    "    df_['menetary_score'] = pd.qcut(df_['monetary'],5,labels=[1,2,3,4,5])\n",
    "    \n",
    "    df_['RFM_SCORE'] = (df_['recency_score'].astype(str)) + (df_['frequency_score'].astype(str))\n",
    "\n",
    "    return df_\n",
    "\n",
    "rfm = get_rfm_scores(rfm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "10개의 등급으로 나누었으면 이는 각 기업의 기준마다 다르다.\n",
    "\n",
    "RFM 3가지 기준에 1~5점의 각각 점수를 부여함\n",
    "recency, frequency, monetary 순\n",
    "최근과 자주 구매하는지만 고려\n",
    "\n",
    "1. 최근에 구매하지 않았고 자주 구매하지 않음 : hibernating(휴면)\n",
    "2. 최근에 구매하지 않았으나 구매시 종종 구매함 : at_Risk(위험)\n",
    "3. 최근에는 구매하지 않았으나 구매 빈도가 아주 높음 : cant_loose(놓쳐서 안되는 고객)\n",
    "4. 2달전 쯤 구매했지만 자주 구매하지 않음 : about_to_sleep(이탈 우려 고객)\n",
    "5. 2달전 쯤 구매했지만 여태 어느정도 자주 구매함 : need_attention(집중 해야할 고객)\n",
    "6. 2달전 쯤 구매하고 자주 물건을 구매함 : loyal_customers(충성 고객)\n",
    "7. 꽤 최근 구매, 처음 구매 : promising(최근 유입 고객)\n",
    "8. 최근 구매, 처음 구매 : new_customers(신규유저)\n",
    "9. 꽤 최근에 구매하고 빈도도 적당함 : potential_loyalists(잠재 우량 고객)\n",
    "10. 최근 구매 다수 구매 : champions(최고 고객)\n",
    "\n",
    "        r'[1-2][1-2]': 'hibernating',\n",
    "           r'[1-2][3-4]': 'at_Risk',\n",
    "           r'[1-2]5': 'cant_loose',\n",
    "           r'3[1-2]': 'about_to_sleep',\n",
    "           r'33': 'need_attention',\n",
    "           r'[3-4][4-5]': 'loyal_customers',\n",
    "           r'41': 'promising',\n",
    "           r'51': 'new_customers',\n",
    "           r'[4-5][2-3]': 'potential_loyalists',\n",
    "           r'5[4-5]': 'champions'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_map = {r'[1-2][1-2]': 'hibernating',\n",
    "           r'[1-2][3-4]': 'at_Risk',\n",
    "           r'[1-2]5': 'cant_loose',\n",
    "           r'3[1-2]': 'about_to_sleep',\n",
    "           r'33': 'need_attention',\n",
    "           r'[3-4][4-5]': 'loyal_customers',\n",
    "           r'41': 'promising',\n",
    "           r'51': 'new_customers',\n",
    "           r'[4-5][2-3]': 'potential_loyalists',\n",
    "           r'5[4-5]': 'champions'}\n",
    "\n",
    "\n",
    "rfm['segment'] = rfm['RFM_SCORE'].replace(seg_map, regex=True)\n",
    "\n",
    "rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = rfm['segment'].value_counts().sort_values(ascending = False)\n",
    "segments.iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treemap 차트로 표현\n",
    "\n",
    "segments = rfm['segment'].value_counts().sort_values(ascending = False)\n",
    "# 밸류의 숫자를 센 세그먼트 컬럼을 변수에 저장 champions = 3090, new_customer= 500 등\n",
    "\n",
    "from plotly import express as px\n",
    "\n",
    "fig = px.treemap(\n",
    "    segments, # 데이터셋\n",
    "    names = list(segments.index),\n",
    "    path= ['segment'], # 데이터 프레임 내 어떤 컬럼을 사용할 것인지\n",
    "    values= segments.values, # 데이터의 값을 비중으로 나타냄\n",
    "    title= 'segments비중' # 차트의 제목을 나타냄\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments = rfm['segment'].value_counts().sort_values(ascending = False)\n",
    "# fig = plt.gcf()\n",
    "# ax = fig.add_subplot()\n",
    "# fig.set_size_inches(16, 10)\n",
    "# squarify.plot(sizes=segments,\n",
    "#               label=[label for label in seg_map.values()],\n",
    "#               color=['#AFB6B5', '#F0819A', '#926717', '#F0F081', '#81D5F0',\n",
    "#                      '#C78BE5', '#748E80', '#FAAF3A', '#7B8FE4', '#86E8C0'],\n",
    "#               pad = False,\n",
    "#               bar_kwargs = {'alpha': 1},\n",
    "#               text_kwargs = {'fontsize':15})\n",
    "# plt.title(\"Customer Segmentation Map\", fontsize = 20)\n",
    "# plt.xlabel('Frequency', fontsize = 18)\n",
    "# plt.ylabel('Recency', fontsize = 18)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (silhouette_score,\n",
    "                             calinski_harabasz_score,\n",
    "                             davies_bouldin_score)\n",
    "\n",
    "X = rfm[['recency_score','frequency_score']]\n",
    "# print(X)\n",
    "\n",
    "labels = rfm['segment']\n",
    "# print(labels)\n",
    "\n",
    "print('관측 개수 : {}'.format(X.shape[0]))\n",
    "print('세그먼트 개수 : {}'.format(labels.nunique()))\n",
    "print('실루엣 계수 : {}'.format(round(silhouette_score(X,labels),3)))\n",
    "\n",
    "# 실루엣 계수(Silhouette Coefficient) : \n",
    "\n",
    "# 각 데이터 포인트와 주위 데이터 포인트들과의 거리 계산을 통해 값을 구하며,\n",
    "# 군집 안에 있는 데이터들은 잘 모여있는지,\n",
    "# 군집끼리는 서로 잘 구분되는지 클러스터링을 평가하는 척도로 활용된다.\n",
    "\n",
    "# * 참고한 논문의 표현을 빌리자면, 군집 내 비유사성('within' dissimilarities)은 작고, \n",
    "# 군집 간 비유사성('between' dissimilarities)은 커야 생성된 클러스터의 품질이 좋다고 할 수 있다. \n",
    "\n",
    "\n",
    "\n",
    "print('칼린스키-하라바즈 스코어 : {}'.format(round(calinski_harabasz_score(X, labels),3)))\n",
    "print('다비스 볼딘 스코어 : {}'.format(round(davies_bouldin_score(X, labels),3)))\n",
    "print(''.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm[['recency','monetary','frequency','segment']].groupby('segment').agg({'mean','std','max','min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 8))\n",
    "ax = sns.countplot(data = rfm,\n",
    "                   x = 'segment',\n",
    "                   palette = palette)\n",
    "total = len(rfm.segment)\n",
    "for patch in ax.patches:\n",
    "    percentage = '{:.1f}%'.format(100 * patch.get_height()/total)\n",
    "    x = patch.get_x() + patch.get_width() / 2 - 0.17\n",
    "    y = patch.get_y() + patch.get_height() * 1.005\n",
    "    ax.annotate(percentage, (x, y), size = 14)\n",
    "plt.title('Number of Customers by Segments', size = 16)\n",
    "plt.xlabel('Segment', size = 14)\n",
    "plt.ylabel('Count', size = 14)\n",
    "plt.xticks(size = 10)\n",
    "plt.yticks(size = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 8))\n",
    "sns.scatterplot(data = rfm, x = 'recency',\n",
    "                y = 'frequency', hue ='segment', palette= palette, s = 60)\n",
    "plt.title('Recency & Frequency by Segments', size = 16)\n",
    "plt.xlabel('Recency', size = 12)\n",
    "plt.ylabel('Frequency', size = 12)\n",
    "plt.xticks(size = 10)\n",
    "plt.yticks(size = 10)\n",
    "plt.legend(loc = 'best', fontsize = 12,\n",
    "           title = 'Segments',title_fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize = (18, 8))\n",
    "fig.suptitle('RFM Segment Analysis', size = 14)\n",
    "feature_list = ['recency', 'monetary', 'frequency']\n",
    "for idx, col in enumerate(feature_list):\n",
    "    sns.boxplot(ax = axes[idx], data = rfm,\n",
    "                x = 'segment', y = feature_list[idx],\n",
    "                palette= palette)\n",
    "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=60)\n",
    "    if idx == 1:\n",
    "        axes[idx].set_ylim([0, 400])\n",
    "    if idx == 2:\n",
    "        axes[idx].set_ylim([0, 30])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "fig.suptitle('RFM Segment Analysis', size = 14)\n",
    "feature_list = ['recency', 'monetary', 'frequency']\n",
    "for idx, col in enumerate(feature_list):\n",
    "    sns.histplot(ax = axes[idx], data = rfm,\n",
    "                 hue = 'segment', x = feature_list[idx],\n",
    "                 palette= palette)\n",
    "    if idx == 1:\n",
    "        axes[idx].set_xlim([0, 400])\n",
    "    if idx == 2:\n",
    "        axes[idx].set_xlim([0, 30])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort Analysis\n",
    "\n",
    "코호트는 앱 가입 날짜, 첫 구매 월, 지리적 위치, 획득 채널(유기 사용자, 공연 마케팅 등) 등 공통점을 공유하는 사람들의 모임이다. 코호트 분석에서, 우리는 몇 가지 일반적인 패턴이나 행동을 식별하기 위해 이러한 사용자 그룹을 시간에 따라 추적한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohortAnalysis(dataframe):\n",
    "    # 1. 데이터프레임을 카피합니다.\n",
    "    data = dataframe.copy() \n",
    "    # 2. 고객아이디, 송장번호, 송장일자에서 중복을 제외한 데이터를 저장합니다.\n",
    "    data = data[['CustomerID','InvoiceNo','InvoiceDate']].drop_duplicates() \n",
    "    # 3. 주문 월이라는 새 컬럼을 생성하고 송장 날짜에서 월만 추출한 데이터를 저장합니다.\n",
    "    data['order_month'] = data['InvoiceDate'].dt.to_period('M')\n",
    "    # 4. 코호트 컬럼을 만듭니다. 고객아이디로 그룹화 한 후 송장일자컬럼의 최저(처음 구매날짜)의 월을 추출\n",
    "    data['cohort'] = data.groupby('CustomerID')['InvoiceDate'].transform('min').dt.to_period('M')\n",
    "    # 5. 고객 아이디의 유니크 값을 세그먼트로 분류한 그룹에서 뽑아냄\n",
    "    cohort_data = data.groupby(['cohort','order_month']).agg(n_customers=('CustomerID','nunique')).reset_index(drop=False)\n",
    "    # cohort_data = data.groupby('segment')['CustomerID'].count()\n",
    "\n",
    "    # 6. \n",
    "    cohort_data['period_number'] = (cohort_data.order_month - cohort_data.cohort).apply(attrgetter('n'))\n",
    "\n",
    "    # 7.\n",
    "    cohort_pivot = cohort_data.pivot_table(\n",
    "        index = 'cohort',\n",
    "        columns = 'period_number',\n",
    "        values = 'n_customers'\n",
    "    )\n",
    "\n",
    "    # 8. \n",
    "    cohort_size = cohort_pivot.iloc[:,0]\n",
    "\n",
    "    # 9.\n",
    "    retention_matrix = cohort_pivot.divide(cohort_size, axis=0)\n",
    "\n",
    "    # 10.\n",
    "    with sns.axes_style('white'):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12,8),\n",
    "        sharey=True,\n",
    "        gridspec_kw ={ 'width_ratios':[1,11]})\n",
    "\n",
    "        sns.heatmap(\n",
    "            retention_matrix,\n",
    "            mask = retention_matrix.isnull(),\n",
    "            annot = True,\n",
    "            cbar = False,\n",
    "            fmt = '.0%',\n",
    "            cmap = 'coolwarm',ax= ax[1]\n",
    "        )\n",
    "\n",
    "        ax[1].set_title('월별 코호트 : 유저 리텐션', fontsize=14)\n",
    "\n",
    "        ax[1].set(\n",
    "            xlabel='기간',\n",
    "            ylabel=' '\n",
    "        )\n",
    "\n",
    "        white_cmap = mcolors.ListedColormap(['white'])\n",
    "        \n",
    "        sns.heatmap(pd.DataFrame(cohort_size).rename(columns={0 : 'cohort_size'}),\n",
    "            annot = True,\n",
    "            cbar = False,\n",
    "            fmt = '0%',\n",
    "            cmap = 'coolwarm', ax = ax[1]\n",
    "        )\n",
    "        \n",
    "        ax[1].set_title('Monthly Cohort: User Retention', fontsize=14)\n",
    "\n",
    "        ax[1].set(xlabel='# of periods', ylabel=' ')\n",
    "\n",
    "        white_cmap = mcolors.ListedColormap(['white'])\n",
    "\n",
    "        sns.heatmap(pd.DataFrame(cohort_size).rename(columns={0:'cohort_size'}),\n",
    "            annot=True,\n",
    "            cbar=False,\n",
    "            fmt='g',\n",
    "            cmap=white_cmap,\n",
    "            ax=ax[0]\n",
    "        )\n",
    "        \n",
    "        fig.tight_layout()\n",
    "\n",
    "cohortAnalysis(df)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CohortAnalysis(dataframe):\n",
    "    \n",
    "    data = dataframe.copy()\n",
    "    data = data[['CustomerID', 'InvoiceNo', 'InvoiceDate']].drop_duplicates()\n",
    "    data['order_month'] = data['InvoiceDate'].dt.to_period('M')\n",
    "    data['cohort'] = data.groupby('CustomerID')['InvoiceDate']\\\n",
    "    .transform('min').dt.to_period('M')\n",
    "    cohort_data = data.groupby(['cohort', 'order_month'])\\\n",
    "    .agg(n_customers=('CustomerID', 'nunique')).reset_index(drop=False)\n",
    "    cohort_data['period_number'] = (cohort_data.order_month - cohort_data.cohort)\\\n",
    "    .apply(attrgetter('n'))\n",
    "    cohort_pivot = cohort_data.pivot_table(index = 'cohort',\n",
    "                                           columns = 'period_number',\n",
    "                                           values = 'n_customers')\n",
    "    cohort_size = cohort_pivot.iloc[:,0]\n",
    "    retention_matrix = cohort_pivot.divide(cohort_size, axis = 0)\n",
    "    with sns.axes_style(\"white\"):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 8),\n",
    "                        sharey=True,\n",
    "                        gridspec_kw={'width_ratios': [1, 11]})\n",
    "        sns.heatmap(retention_matrix, \n",
    "                    mask = retention_matrix.isnull(), \n",
    "                    annot = True,\n",
    "                    cbar = False,\n",
    "                    fmt='.0%', \n",
    "                    cmap='coolwarm', ax=ax[1])\n",
    "        ax[1].set_title('Monthly Cohorts: User Retention', fontsize=14)\n",
    "        ax[1].set(xlabel='# of periods',\n",
    "                  ylabel='')\n",
    "        white_cmap = mcolors.ListedColormap(['white'])\n",
    "        sns.heatmap(pd.DataFrame(cohort_size).rename(columns={0: 'cohort_size'}), \n",
    "                    annot=True, \n",
    "                    cbar = False,\n",
    "                    fmt='g',\n",
    "                    cmap=white_cmap,\n",
    "                    ax=ax[0])\n",
    "        fig.tight_layout()\n",
    "    \n",
    "CohortAnalysis(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Lifetime Value\n",
    "고객 평생 가치는 고객이 지불 고객으로서 전체 기간 동안 얼마나 많은 돈을 브랜드에 가져다 줄 것인가?\n",
    "\n",
    "https://assaeunji.github.io/statistics/2022-04-15-ltv/\n",
    "\n",
    "LTV는 확률 기반 모형으로 특정한 시점 t에서 고객마다 어느정도의 생애가치를 가져다 주는지 측정\n",
    "\n",
    "고객의 과거 구매정보를 가지고 미래의 예상 구매횟수 * 예상 평균수익으로 계산\n",
    "----\n",
    "\n",
    "고객의 과거 구매정보 :  \n",
    "    1. 첫 구매 ~ 집계일까지의 시간(T)\n",
    "    2. 첫 구매 ~ 마지막 구매일자까지의 시간(Recency ; tx - t0)\n",
    "    3. 첫 구매 ~ 집계일까지의 구매 횟수(Frequency ; x)\n",
    "    4. 구매 건 마다의 구매 금액(Monetary Value ; mx)\n",
    "\n",
    "미래의 예상 구매횟수 RFM분석을 통한 데이터를 바탕으로 BG/NBD 모델로 추정\n",
    "\n",
    "미래의 예상 구매금액은 Monetary Value의 정보를 가지고 Gamma-Gamma모델로 추정\n",
    "\n",
    "앞에서 확률적으로 모델을 추정한다 말씀을 드렸는데요. 모델 이름도 사실 확률 분포의 이름에서 따왔습니다. \n",
    "BG/NBD는 베타 분포와 기하 분포의 결합과, 음이항 분포로 구성되고(Beta-Geometric / Negative Binoimal Distribution), \n",
    "Gamma-Gamma는 두 감마 분포의 결합으로 이루어집니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTYD 모델\n",
    "\n",
    "구체적인 모형을 설명하기에 앞서 “죽을 때까지 구매하는” (BTYD; Buy Till You Die) 모형어감이 그러니 이제부터 BTYD 모형이라 부르겠습니다의 컨셉부터 설명드리겠습니다. BTYD 모형을 바탕으로 BG/NBD 모형을 구성합니다.\n",
    "\n",
    "BTYD 모델은 과거의 구매 횟수와 구매 금액과 같은 데이터를 바탕으로 고객의 생애 가치를 계산합니다. BTYD는 다음과 같은 질문에 답할 수 있습니다.\n",
    "\n",
    "현재 얼마나 많은 고객들이 남아있나요?\n",
    "지금으로부터 1년 뒤에 얼마나 많은 고객들이 남아있을까요?\n",
    "어떤 고객들이 이탈했었나요?\n",
    "고객들이 한 회사에 미래에 얼마를 소비할까요?\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "1. 구매 (Transaction) 프로세스 (Buy)\n",
    "\n",
    "고객이 남아있는 동안, 고객의 구매 횟수는 일정한 기간 동안의 구매율을 모수로 하는 포아송 분포를 따릅니다. 사실 포아송 분포는 “구매 횟수”에 대한 분라 구매할 확률을 모수로 하지 않고, 정해진 시간 동안의 예상 구매 횟수를 모수로 하는데요.\n",
    "\n",
    "여기서 전 왜 원 논문에서는 모수를 구매 횟수가 아닌 구매율 (transaction rate)로 표현하는가가 이해가 되지 않았었죠. 아마도 일정한 기간을 어떻게 정의하느냐에 따라서 충분히 비율로 표현할 수 있기 때문이라 생각이 듭니다. 예를 들어, 1년에 30번 구매한 고객이 있다면 구매 횟수가 Pois (30)을 따른다고 말할 수도 있겠지만, 단위 기간을 이보다 더 좁은 기간인 1일로 정의한다면 1일에 (30 / 12개월) / 30일 = 1/12의 확률로 구매를 한다 말할 수 있기 때문에 Pois(1/12)라고 말할 수 있는 것이죠.\n",
    "\n",
    "이 구매율은 고객마다 다르다고 가정하고, 전체 고객들의 구매율은 Gamma(r,α)를 따른다 가정합니다.\n",
    "\n",
    "2. 이탈 (Dropout) 프로세스 (Till You Die)\n",
    "\n",
    "각 고객들은 이탈률 p를 가집니다. 즉, 고객들은 구매를 한 후에 특정한 확률로 이탈함을 가정하는 것입니다. 이러한 이탈 확률은 고객마다 달라 Beta (a,b)를 따른다 가정합니다. 보통 확률에 대한 분포를 가정할 때 범위가 (0,1)로 정해져있는 베타 분포를 가정하곤 하는데, 감마 분포를 가정한 이유도 위와 같은 이유이지 않을까 생각해봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 말했듯 BG/NBD 모형은 BTYD 모형에 기반하여 분포를 가정합니다. 구체적으로 분포 가정은 다음과 같습니다.\n",
    "\n",
    "1. 고객이 남아있는 동안, 일정한 기간 T 동안의 구매 횟수는 Pois(λT)를 따릅니다. 위에서 말씀 드렸듯이 1일 간 Pois (1/12)를 따른다면 T= 1년일 경우 Pois (30)을 따르게 되겠죠! (포아송의 모수인 λT는 예상 구매 횟수 를 의미합니다)\n",
    "\n",
    "2. 고객마다 일정한 기간 동안 구매하는 횟수는 다릅니다. 이는 λ~ Gamma (r, α)을 따릅니다.\n",
    "\n",
    "3. j번째 구매가 마지막이고 더 이상 구매를 하지 않을 확률 (이탈률)은 p입니다. 이탈할 때까지의 구매 횟수는 Geo (p)를 따릅니다.\n",
    "\n",
    "4. 고객마다 더 이상 구매를 하지 않을 확률 (이탈률)은 다릅니다. 이탈률 p는 p ~ Beta (a,b)를 따릅니다.\n",
    "\n",
    "5. 고객별 일정 기간 동안의 구매 횟수와 구매를 하지 않을 확률은 서로 영향을 주지 않습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "모형 이름이 BG/NBD인데 \n",
    "\n",
    "B에 해당하는 베타 분포 (Beta Distribution), \n",
    "G에 해당하는 기하 분포 (Geometric Distribution)\n",
    "\n",
    "NBD에 해당하는 음이항 분포 (Negative Binomial Distribution) \n",
    "\n",
    "포아송 분포와 감마 분포의 결합이 바로 음이항 분포 (Negative Binomial Distribution)\n",
    "\n",
    "이를 Poisson-Gamma Mixture라 부릅니다.\n",
    "\n",
    "포아송 분포: 단위 시간 동안의 성공 횟수에 대한 분포\n",
    "감마 분포: 사건을 n번 시행할 때까지의 총 시간에 대한 분포\n",
    "기하 분포: 사건이 1번 발생할 때까지의 시도 횟수에 대한 분포\n",
    "베타 분포: 확률에 대한 분포 (범위: (0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "날짜\t구매 금액\n",
    "2022.01.16\t44,159원\n",
    "2022.01.24\t44,385원\n",
    "2022.02.03\t40,700원\n",
    "2022.02.05\t43,520원\n",
    "2022.02.26\t48,140원\n",
    "2022.03.16\t27,186원\n",
    "2022.03.23\t37,161원\n",
    "2022.04.10\t40,060원\n",
    "\n",
    "\n",
    "\n",
    "포아송 분포: 네 달에 거쳐 8번 구매했으니 한 달에 2번 구매한 꼴이 됩니다. 이처럼 한 달의 평균 구매 횟수를 Pois (λ)를 따른다 가정합니다.\n",
    "\n",
    "감마 분포: 위 내용을 바꿔 말하면 두 번 구매할 때까지 한 달이라는 시간이 걸렸다고 말할 수 있습니다. 이처럼 r번 구매할 때까지 걸리는 시간은 Gamma(r,α)를 따른다 가정합니다.\n",
    "\n",
    "기하 분포: 여기서의 사건은 “이탈”입니다. 이탈할 때까지의 총 구매 횟수는 Geo(p)를 따른다 가정합니다.\n",
    "\n",
    "베타 분포: 마지막으로 이 이탈률 p는 Beta(a,b)를 따른다 가정합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BG/NBD 모델은 고객마다 다른 구매 횟수를 모델링했다면, Gamma-Gamma 모델은 고객별 구매 금액 정보들을 이용합니다.\n",
    "\n",
    "\n",
    "Gamma-Gamma 모델은 다음과 같은 가정을 하게 됩니다.\n",
    "\n",
    "1. 고객별 구매 금액은 평균 구매 금액을 중심으로 랜덤하게 분포합니다.\n",
    "2. 고객들의 평균 구매 금액은 고객마다 다르지만, 한 고객의 평균 구매 금액은 시간에 따라 불변합니다.\n",
    "3. 이 평균 구매 금액은 구매 과정과 독립이여야 합니다.\n",
    "\n",
    "\n",
    "이에 따라 다음과 같이 분포를 가정하여 가능도를 구하게 됩니다.\n",
    "\n",
    "1. 각 구매 건별 구매 금액 zi z i 는 zi∼ z i ∼  Gamma (p,v)를 따른다 가정합니다. \n",
    "총 x번 구매하면 ∑xi=1zi∼ ∑ i = 1 x z i ∼  Gamma (px,v)를 따르고, 평균 구매 금액인 mx=∑xi=1zi/x m x = ∑ i = 1 x z i / x 는 ∼ ∼  Gamma(px, vx)을 따르게 됩니다. \n",
    "(Gamma 분포를 따르는 확률 변수의 합도 Gamma분포를 따릅니다. 자세한 내용은 위키 참조!) \n",
    "\n",
    "2. 고객마다 평균 구매 금액이 다르기 때문에 scale 모수인 v 또한 v∼ v ∼ Gamma (q,γ γ )를 따른다 가정합니다. \n",
    "\n",
    "3. 이 둘의 관계를 갖고 베이즈 정리를 활용한다면 평균 구매 금액의 marginal 분포는 Gamma (px+q, γ+mxx γ + m x x )를 따르게 됩니다. (mx=∑xi=1zi/x m x = ∑ i = 1 x z i / x  평균 구매 금액)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고객마다 학습에 들어가는 T (Time), R (Recency), F (Frequency),M (Monetary) 정보와 결합하여\n",
    "\n",
    "구매 빈도 (F)는 낮은데 LTV가 높은 고객\n",
    "구매 빈도 (F), 평균 구매 금액 (M)이 높고 LTV가 높은 고객\n",
    "최근성 (R)은 떨어지는데, LTV가 높은 고객\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Lifetime Value\n",
    "\n",
    "\n",
    "cltv_df = df.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': [lambda x : (x.max() - x.min()).days,\n",
    "                    lambda x : (today_date - x.min()).days], # 이런식으로 리스트 안에 두가지 식을 적용 시 컬럼이 각각 만들어짐\n",
    "    'InvoiceNo': lambda x : x.nunique(),\n",
    "    'TotalPrice' : lambda x : x.sum()\n",
    "})\n",
    "\n",
    "# cltv_df.head()\n",
    "# InvoiceDate\tInvoiceNo\tTotalPrice\n",
    "# <lambda_0>\t<lambda_1>\t<lambda>\t<lambda>\n",
    "\n",
    "# T 컬럼은 오늘부터 가장 처음 구매한 기간의 차로 구매 기간을 의미\n",
    "cltv_df.columns = cltv_df.columns.droplevel(0)\n",
    "cltv_df.columns = ['recency', 'T', 'frequency', 'monetary']\n",
    "cltv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Order Value\n",
    "cltv_df['monetary'] = cltv_df['monetary'] / cltv_df['frequency']\n",
    "\n",
    "# Recency & Tenure\n",
    "cltv_df['recency'] = cltv_df['recency'] / 7\n",
    "cltv_df['T'] = cltv_df['T'] / 7\n",
    "\n",
    "# Frequency\n",
    "cltv_df = cltv_df[(cltv_df['frequency'] > 1)]\n",
    "\n",
    "cltv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BG/NBD\n",
    "\n",
    "\n",
    "베타 기하학적 / 음의 이항 분포 모델은 각 고객이 수행한 트랜잭션 수가 감마 분포에 이어 고객 간 트랜잭션 속도의 이질성을 갖는 포아송 프로세스를 따른다고 가정합니다. \n",
    "\n",
    "이러한 가정은 고객이 살아있는 동안 수행한 트랜잭션 수를 모델링하기 위한 음의 이항 분포(NBD)를 제공합니다. \n",
    "우리는 BetaGeoFitter와 Life times 패키지를 사용하여 BG/NBD 모델을 구축할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGF = BetaGeoFitter(penalizer_coef= 0.001) #과대적합 방지\n",
    "\n",
    "BGF.fit(cltv_df['frequency'],\n",
    "        cltv_df['recency'],\n",
    "        cltv_df['T'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGF.conditional_expected_number_of_purchases_up_to_time(1,\n",
    "                                                        cltv_df['frequency'],\n",
    "                                                        cltv_df['recency'],\n",
    "                                                        cltv_df['T']\n",
    "                                                       ).sort_values(ascending = False)\\\n",
    ".head(10)\\\n",
    ".to_frame('Expected Number of Transactions')\\\n",
    ".reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGF.conditional_expected_number_of_purchases_up_to_time(4,\n",
    "                                                        cltv_df['frequency'],\n",
    "                                                        cltv_df['recency'],\n",
    "                                                        cltv_df['T']\n",
    "                                                       ).sort_values(ascending = False)\\\n",
    ".head(10)\\\n",
    ".to_frame('Expected Number of Transactions')\\\n",
    ".reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a figure with period actual and predicted transactions.\n",
    "plot_period_transactions(BGF,\n",
    "                         max_frequency = 7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9b23877bf6d73814302983349e2991ec844e3cd0032c827dc21d92f1bfb4419"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
